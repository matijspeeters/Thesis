\chapter{Development process}

\section{Diffusion model used} \label{sec:Models used}
\subsection{FLUX.1 [dev]}
The diffusion model used in this thesis is called FLUX.1 (\cite{black_forest_labs_announcing_2024}), a text-to-image diffusion model released by Black Forest Labs on the first of August 2024. Of its three variants, FLUX.1 [pro], [dev], and [schnell], only [dev] and [schnell] have open-source weights and thus support local use and fine-tuning via low-rank adaptation (LoRA).
Following preliminary experimentation, only FLUX.1 [dev] is used in this thesis, because of its superior integration with Ostris' FLUX.1 [dev] LoRA trainer. LoRAs trained with the [dev] trainer also function with FLUX.1 [schnell], but yield lower-quality output images. Training LoRAs specifically for FLUX.1 [schnell] is also possible, although one would need to use an adapter for that (the adapter can be found on \href{https://huggingface.co/ostris/FLUX.1-schnell-training-adapter}{https://huggingface.co/ostris/FLUX.1-schnell-training-adapter}). This option was not tested.
\section{Image generation techniques used}
\subsection{Text-to-Image}
\subsubsection{ChatGPT} \label{sec:ChatGPT}
To streamline the prompt engineering process, the researcher trained a custom GPT named 'LoRA prompt generator' (\href{https://chatgpt.com/g/g-68279d68896c81918191491b79281abe-lora-prompt-generator}{link}) to output high-quality prompts consisting of a six-part structure. A GPT is a custom version of ChatGPT that can follow specific instructions (\cite{openai_introducing_2023}). This option was chosen because of the combination of accessibility (since GPTs are easy to configure without knowing a lot about large language models) and effectivity.\\
The recommended models to use for this GPT are GPT-o3 and GPT-o4-mini, because they are able to directly integrate images into their chain of thought (\cite{openai_introducing_2025}, section 'Thinking with images').\\
The researcher can write a prompt in natural language, and the GPT reformulates it into a six-part prompt following a fixed structure. This structure facilitates easier analysis of the prompts. Table \ref{tab:gptprompt} dissects a representative output generated by the GPT to illustrate the structure. \\

\begin{table}[H]
  \centering
  \begin{tabular}{l p{0.6\textwidth}}
    \toprule
    Structure & Prompt segment \\
    \midrule
    1. Trigger word 
    & Stampbeton \\
    2. Subject
    & shapes the rugged elegance of a countryside farmhouse, where the front facade stands defined by sturdy stamped concrete walls that echo the texture of hand-laid stone. \\
    3. Attributes on the subject
    & A gently slanted roof clad in red shingles crowns the structure, lending a traditional warmth that contrasts the cool, tactile mass of the concrete below. On the front elevation, a single wooden door punctuates the left side, its weathered grain harmonizing with the rustic palette, while a solitary window on the right offers a glimpse into the homeâ€™s quiet interior, framed by thick concrete reveals. \\
    4. Surroundings
    & The house is nestled into a soft rural clearing, flanked by wild grasses and a gravel path that leads up to the entrance, with distant hedgerows and fencing marking the rhythm of nearby fields. \\
    5. Atmosphere 
    & Overhead, a wide expanse of blue sky peeks through the rolling coverage of dense, moody clouds, diffusing the daylight into a serene, shadowless glow. \\
    6. Style 
    & Rendered in photorealistic style, the scene captures the raw materiality and quiet strength of this rural retreat, balanced by a subtle, cinematic atmosphere. \\
    \bottomrule
  \end{tabular}
  \caption{The six-part structure used to generate every prompt in this thesis.\\}
  \label{tab:gptprompt}
\end{table}
Users can edit the output prompt to their preference, reducing the workload compared to writing a high-quality prompt from scratch. \\
Because every image generated in this thesis is a pair (with and without LoRA; see section \ref{sec:Workflow used to generate images}), the GPT was designed to output two prompts: one with the trigger words and one with their English equivalents that the base model FLUX.1 [dev] can interpret. This ensures the comparison is as fair as possible.

\begin{table}[H]
    \centering
    \begin{tabular}{ll}
        \toprule
         Stampbeton & Stamped concrete\\
         3Deffect & 3D effect\\
         Geleding & Division \\
         Modulariteit & Modularity\\
         Ghoek & Curvature\\
         Plintwerking & Plinth effect\\
         \bottomrule
    \end{tabular}
    \caption{All of the trigger words and their English counterpart.}
    \label{tab:triggerwords}
\end{table}

\subsection{Image-to-Image}
\subsection{ControlNet}
To generate images based on sketches or images of the architects, \textbf{ControlNet} (\cite{zhang_adding_2023}) was used, a technique to add compositional control to large diffusion models such as FLUX. The ControlNet model used in this thesis is the 'union pro' model by Shakker Labs (\href{}{})
\section{Interface used to generate images} \label{sec:Workflow used to generate images}
\subsection{ComfyUI}
\sloppy
The interface used is called \textbf{ComfyUI} (\href{https://www.comfy.org/}{comfy.org}), an open source node-based user interface. Alternatives such as Automatic1111, Fooocus and InvokeAI were considered; however, ComfyUI was selected due to two primary advantages.\\ 
1.\textbf{Retraceability}: generated images embed the complete 'workflow' used to create it (see \href{https://docs.comfy.org/development/core-concepts/workflow}{docs.comfy.org} for further explanations), allowing for easy reproduction. This is an important aspect for research, as it allows researchers to retrace how every AI-generated image in this thesis was created.\\
2.\textbf{Customizability}: by using fundamental building blocks ('nodes', see \href{https://docs.comfy.org/development/core-concepts/nodes}{docs.comfy.org}), ComfyUI users can create their own workflows to meet various needs, and can also write and implement custom nodes to further customize workflows.
Figure \ref{fig:comfy interface} illustrates the default ComfyUI workflow, which can generate one image from a positive and a negative prompt.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Images//Methodology/comfyui_new_interface.jpg}
    \caption{The default workflow when starting ComfyUI.}
    \label{fig:comfy interface}
\end{figure}

To generate images, an online service called ThinkDiffusion (\href{https://www.thinkdiffusion.com/}{thinkdiffusion.com}) was used. With Thinkdiffusion, one can rent private online servers strong enough to run diffusion models.\\
A server with 48 GB of vram was rented, which is capable of generating one image in about 30 seconds when using FLUX.1 [dev]. The version of ComfyUI used is ComfyUI v0.3.44, the latest available in July 2025.
\subsection{Workflow used in this thesis}
To generate all of the images in this thesis, a custom workflow was downloaded (\cite{sebastian_kamph_flux_2024}) which allows the use of ControlNets in combination with FLUX.
\subsubsection{Comparison between LoRA and baseline}
To isolate and highlight the effect of LoRA on the generated images, the researcher modified the custom workflow to generate \textbf{two images per execution}: one with LoRA enabled and one without. All other parameters, such as prompts and seed values, were held constant.\\
Figure \ref{fig:own comfy workflow} shows the custom workflow in ComfyUI. Every time a request to generate images is ran, two images appear in the nodes on the bottom right.\\ 
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Images/Methodology/workflow (5).png}
    \caption{The developed ComfyUI workflow used in this thesis. The image and the workflow can be downloaded \href{https://github.com/matijspeeters/Thesis_Lora/blob/7aaf540acbfc18f98d5cf12d135511dd9963f9fc/ComfyUI_workflow.png}{here}.}
    \label{fig:own comfy workflow}
\end{figure}